{"docstore/metadata": {"6442ccb7-7433-4b9f-a3f5-bb92498d8bc2": {"doc_hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716"}, "e3565792-87e8-46fd-93a5-a728c849291f": {"doc_hash": "9c1aa40ecbaf65da717e6ce8d2e50a14e9f3db024d6acf0715b25f6ef2bf2dab", "ref_doc_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2"}, "cb49b3ed-4801-4e0b-812c-8f1548517709": {"doc_hash": "d917ab733949f0f323512d757123f1cb944fb5906bdd1602c9779670b833c0f9", "ref_doc_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2"}, "c231e247-5deb-4758-9e4f-0a16d517e979": {"doc_hash": "dc0b378bef3de3cceb8d0bfb7533fa288a697b1ae8f0d4b1fed3c398aeb2f0dc", "ref_doc_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2"}, "0aa4c12d-2b6b-43f2-90c1-73ca56d44052": {"doc_hash": "be200c2e05dc2f6c4c1637f7326cd3945a1b021f5ea71177a4a073275f28bdab", "ref_doc_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2"}, "2cad8024-1554-4ed0-b629-7bf7b1c9e15b": {"doc_hash": "2f0ef3bc338372ae00c243380d0517df761c66ad4bb0f469443bd49f500d2368", "ref_doc_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2"}, "447e369d-03e2-4097-8a05-b8500e4555bd": {"doc_hash": "4e044ed2990ea040e3a3a151406df61fd9333c82ac373d8885a9ce1e52906059", "ref_doc_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2"}}, "docstore/data": {"e3565792-87e8-46fd-93a5-a728c849291f": {"__data__": {"id_": "e3565792-87e8-46fd-93a5-a728c849291f", "embedding": null, "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2", "node_type": "4", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb49b3ed-4801-4e0b-812c-8f1548517709", "node_type": "1", "metadata": {}, "hash": "764cf87ddeedd47da336ee56863939277b759405eba390e7b012ca555fc44bdf", "class_name": "RelatedNodeInfo"}}, "text": "Search\nWrite\n\nTHOMAS ANTONY\nGet unlimited access to the best of Medium for less than $1/week.\nBecome a member\n\n\nTop highlight\n\nArtificial Neural Network (ANN) with Practical Implementation\nAmir Ali\nThe Art of Data Scicne\nAmir Ali\n\n\u00b7\nFollow\n\nPublished in\nThe Art of Data Scicne\n\n\u00b7\n15 min read\n\u00b7\nMay 20, 2019\n167\n\n\n1\n\n\n\n\n\n\nIn this Second Chapter of Deep Learning, we will discuss the Artificial Neural Network. It is a Supervised Deep Learning technique and we will discuss both theoretical and Practical Implementation from Scratch.\n\nThis chapter spans 5 parts:\n\nWhat is an Artificial Neural Network?\nTypes of Artificial Neural Network.\nActivation Functions and There types?\nHow Do Backpropagation works?\nPractical Implementation of ANN in Keras and Tensorflow.\n1. What is an Artificial Neural Network?\nArtificial neural networks are one of the main tools used in machine learning. As the \u201cneural\u201d part of their name suggests, they are brain-inspired systems that are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns that are far too complex or numerous for a human programmer to extract and teach the machine to recognize.\n\nWhile neural networks (also called \u201cperceptrons\u201d) have been around since the 1940s, it is only in the last several decades where they have become a major part of artificial intelligence. This is due to the arrival of a technique called \u201cbackpropagation,\u201d which allows networks to adjust their hidden layers of neurons in situations where the outcome doesn\u2019t match what the creator is hoping for \u2014 like a network designed to recognize dogs, which misidentifies a cat, for example.\n\nAnother important advance has been the arrival of deep learning neural networks, in which different layers of a multilayer network extract different features until it can recognize what it is looking for.\n\n1.1: Basic Structure of ANNs\nThe idea of ANNs is based on the belief that working of the human brain by making the right connections can be imitated using silicon and wires as living neurons and dendrites.\n\nThe human brain is composed of 86 billion nerve cells called neurons. They are connected to other thousand cells by Axons. Stimuli from the external environment or inputs from sensory organs are accepted by dendrites. These inputs create electric impulses, which quickly travel through the neural network. A neuron can then send the message to another neuron to handle the issue or does not send it forward.\n\n\nANNs are composed of multiple nodes, which imitate biological neurons of the human brain. The neurons are connected by links and they interact with each other. The nodes can take input data and perform simple operations on the data. The result of these operations is passed to other neurons. The output at each node is called its activation or node value.\n\nEach link is associated with weight. ANNs are capable of learning, which takes place by altering weight values. The following illustration shows a simple ANN \u2013\n\n\n2. Types of Artificial Neural Networks\nThere are two Artificial Neural Network topologies \u2212 FeedForward and Feedback.\n\n2.1: FeedForward ANN\n\nIn this ANN, the information flow is unidirectional. A unit sends information to another unit from which it does not receive any information. There are no feedback loops. They are used in pattern generation/recognition/classification. They have fixed inputs and outputs.\n\n\n2.2: FeedBack ANN\n\nHere, feedback loops are allowed. They are used in content-addressable memories.", "start_char_idx": 1, "end_char_idx": 3695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb49b3ed-4801-4e0b-812c-8f1548517709": {"__data__": {"id_": "cb49b3ed-4801-4e0b-812c-8f1548517709", "embedding": null, "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2", "node_type": "4", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3565792-87e8-46fd-93a5-a728c849291f", "node_type": "1", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9c1aa40ecbaf65da717e6ce8d2e50a14e9f3db024d6acf0715b25f6ef2bf2dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c231e247-5deb-4758-9e4f-0a16d517e979", "node_type": "1", "metadata": {}, "hash": "eeb4f148afe2cf77202919345f000c602fa4b39d48ca85ec72cf249eb8e07ebb", "class_name": "RelatedNodeInfo"}}, "text": "3. Activation Functions and There Types?\n3.1: What is Activation Function?\nIt\u2019s just a thing function that you use to get the output of the node. It is also known as Transfer Function.\n\nActivation functions are really important for an Artificial Neural Network to learn and make sense of something reallocated and Non-linear complex functional mappings between the inputs and response variable. They introduce non-linear properties to our Network. Their main purpose is to convert an input signal of a node in an ANN to an output signal. That output signal now is used as an input in the next layer in the stack.\n\nSpecifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights (W) and apply an Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n\n3.2: Types of activation Functions?\nIt is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n\nThe Activation Functions can be based on 2 types-\n\n1. Linear Activation Function\n\n2. Non-linear Activation Functions\n\nLinear Activation Function\n\nAs you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range.\n\n\nFig: Linear Activation Function\nEquation: f(x) = x\n\nRange: (-infinity to infinity)\n\nIt doesn\u2019t help with the complexity of various parameters of usual data that is fed to the neural networks.\n\nNon-linear Activation Function\nThe Nonlinear Activation Functions are the most used activation functions. Nonlinearity helps to makes the graph look something like this\n\n\nFig: Non-linear Activation Function\nIt makes it easy for the model to generalize or adapt to a variety of data and to differentiate between the outputs.\n\nThe main terminologies needed to understand for nonlinear functions are:\n\nDerivative or Differential: Change in y-axis w.r.t. change in the x-axis. It is also known as a slope.\n\nMonotonic function: A function that is either entirely non-increasing or non-decreasing.\n\nThe Nonlinear Activation Functions are mainly divided on based on range or curves-\n\nDifferent Types of Activation function in non-Linear\n\n1. Sigmoid Activation Function\n\nThe Sigmoid Function curve looks like an S-shape.\n\n\nSigmoid Function\nThe main reason why we use the sigmoid function is that it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since the probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n\nThe function is differentiable. That means, we can find the slope of the sigmoid curve at any two points.\n\nThe function is monotonic but the function\u2019s derivative is not.\n\nThe logistic sigmoid function can cause a neural network to get stuck at the training time.\n\nThe softmax function is a more generalized logistic activation function that is used for multiclass classification.\n\n2. Tanh Activation Function\n\ntanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s-shaped).\n\n\ntanh\nThe advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. The function is differentiable.\n\nThe function is monotonic while its derivative is not monotonic.\n\nThe tanh function is mainly used classification between two classes.\n\nBoth tanh and logistic sigmoid activation functions are used in feed-forward nets.\n\n3. ReLU (Rectified Linear Unit) Activation Function\n\nThe ReLU is the most used activation function in the world right now. Since it is used in almost all the convolutional neural networks or deep learning.", "start_char_idx": 3698, "end_char_idx": 7465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c231e247-5deb-4758-9e4f-0a16d517e979": {"__data__": {"id_": "c231e247-5deb-4758-9e4f-0a16d517e979", "embedding": null, "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2", "node_type": "4", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb49b3ed-4801-4e0b-812c-8f1548517709", "node_type": "1", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "d917ab733949f0f323512d757123f1cb944fb5906bdd1602c9779670b833c0f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0aa4c12d-2b6b-43f2-90c1-73ca56d44052", "node_type": "1", "metadata": {}, "hash": "74826e1683be4c1eb5a849186b501acdf2f650a95fd5182d82271c8bb055c4f0", "class_name": "RelatedNodeInfo"}}, "text": "Relu\nAs you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.\n\nRange: [ 0 to infinity)\n\nThe function and it is derivative both are monotonic.\n\nBut the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turn affects the resulting graph by not mapping the negative values appropriately.\n\n4: How Backpropagation works?\nLet\u2019s dive into the mathematics part and see the backpropagation working with feedforward and feed backward Neural Network in depth.\n\nApproach\n\nStep 1: Randomly initialize the Weights to a small number close to 0 (but not 0).\n\nStep 2: Input the first observation of your dataset in the input layer, each feature in one input node.\n\nStep 3: Forward-Propagation: from left to right, the neurons are activated in a way that the impact of each neuron\u2019s activation is limited by the weights. Propagates the activations until getting the predicted result y.\n\nStep 4: Compare the predicted result to the actual result. Measure the generated error.\n\nStep 5: Back-Propagation: from left to right, the error is back-Propagated. Update the weights according to how much they are responsible for the error. The Learning rate decides how much we update the weights.\n\nStep 6: Repeat step 1 to step 5 and updates the weights after each observation.\n\nStep 7: When the whole training set passed through the ANN that makes an epoch. Redo more epoch.\n\nArchitecture\n1. Build a Feed-Forward neural network with 2 hidden layers. All the layers will have 3 Neurons each.\n\n2. 1st and 2nd hidden layers will have RELU and sigmoid respectively as activation functions. The final layer will have a Softmax activation function.\n\n3. Error is calculated using cross-entropy.\n\n\n4. How Does Backpropagation work?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo Weights are updated in each layer. Again forward pass then recalculates the errors with actual output and then backward pass repeat the step until error is reduced.\n\nSo in these ways backpropagation algorithm work.\n\nNote: If you want this article check out my academia.edu profile.\n\n5. Practical Implementation of Artificial Neural Network?\nChurn Modelling Problem\nIn this part, you will be solving a data analytics challenge for a bank. You will be given a dataset with a large sample of the bank\u2019s customers. To make this dataset, the bank gathered information such as customer id, credit score, gender, age, tenure, balance, if the customer is active, has a credit card, etc. During 6 months, the bank observed if these customers left or stayed in the bank.\n\nYour goal is to make an Artificial Neural Network that can predict, based on geo-demographical and transactional information given above, if any individual customer will leave the bank or stay (customer churn). Besides, you are asked to rank all the customers of the bank, based on their probability of leaving. To do that, you will need to use the right Deep Learning model, one that is based on a probabilistic approach.\n\nIf you succeed in this project, you will create significant added value to the bank. By applying your Deep Learning model the bank may significantly reduce customer churn.\n\nDataset sample:", "start_char_idx": 7468, "end_char_idx": 10898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0aa4c12d-2b6b-43f2-90c1-73ca56d44052": {"__data__": {"id_": "0aa4c12d-2b6b-43f2-90c1-73ca56d44052", "embedding": null, "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2", "node_type": "4", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c231e247-5deb-4758-9e4f-0a16d517e979", "node_type": "1", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "dc0b378bef3de3cceb8d0bfb7533fa288a697b1ae8f0d4b1fed3c398aeb2f0dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cad8024-1554-4ed0-b629-7bf7b1c9e15b", "node_type": "1", "metadata": {}, "hash": "1012b74cc7c84d7f2c126e634b5d76f20047c41fb8206143abb49067aa95928d", "class_name": "RelatedNodeInfo"}}, "text": "Part 1: Data Preprocessing\n\n1.1 Import the Libraries\n\nIn this step, we import three Libraries in Data Preprocessing part. A library is a tool that you can use to make a specific job. First of all, we import the numpy library used for multidimensional array then import the pandas library used to import the dataset and in last we import matplotlib library used for plotting the graph.\n\n\n1.2 Import the dataset\n\nIn this step, we import the dataset to do that we use the pandas library. After import our dataset we define our dependent and independent variable. Our independent variables are 1 to 12 attributes as you can see in the sample dataset which we call \u2018X\u2019 and dependent is our last attribute which we call \u2018y\u2019 here.\n\n\n1.3 Encoding the Categorical data\n\nIn this step, we Encode our categorical data. If we see our dataset then Geography & Gender attribute is in Text and we Encode these two attributes in this part use the LabelEncoder and OneHOTEncoder from the Sklearn.processing library.\n\n\n1.4 Split the dataset for test and train\n\nIn this step, we split our dataset into a test set and train set and an 80% dataset split for training and the remaining 20% for tests. Our dataset contains 10000 instances so 8000 data we train and 2000 for the test.\n\n\n1.5 Feature Scaling\n\nFeature Scaling is the most important part of data preprocessing. If we see our dataset then some attribute contains information in Numeric value some value very high and some are very low if we see the age and estimated salary. This will cause some issues in our machinery model to solve that problem we set all values on the same scale there are two methods to solve that problem first one is Normalize and Second is Standard Scaler.\n\n\nHere we use standard Scaler import from Sklearn Library.\n\n\nPart 2: Building our Model\n\nIn this part, we model our Artificial Neural Network model.\n\n2.1 Import the Libraries\n\nIn this step, we import the Library which will build our ANN model. We import Keras Library which will build a deep neural network based on Tensorflow because we use Tensorflow backhand. Here we import the two modules from Keras. The first one is Sequential used for initializing our ANN model and the second is Dense used for adding different layers of ANN.\n\n\n2.2 Initialize our ANN model\n\nIn this step, we initialize our Artificial Neural Network model to do that we use sequential modules.\n\n\n2.3 Adding the input layer and first hidden layer\n\nIn this step, we use the Dense model to add a different layer. The parameter which we pass here first is output_dim=6 which defines hidden layer=6, the second parameter is init= uniform basically this is a uniform function that randomly initializes the weights which are close to 0 but not 0. The third parameter is activation= relu here in the first hidden layer we use relu activation. And the last parameter which we pass in dense function is input_dim= 11 which means the input node of our Neural Network is 11 because our dataset has 11 attributes that\u2019s why we choose 11 input nodes.\n\n\n2.4 Adding the Second Hidden layer\n\nIn this step, we add another hidden layer", "start_char_idx": 10901, "end_char_idx": 14011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cad8024-1554-4ed0-b629-7bf7b1c9e15b": {"__data__": {"id_": "2cad8024-1554-4ed0-b629-7bf7b1c9e15b", "embedding": null, "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2", "node_type": "4", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0aa4c12d-2b6b-43f2-90c1-73ca56d44052", "node_type": "1", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "be200c2e05dc2f6c4c1637f7326cd3945a1b021f5ea71177a4a073275f28bdab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "447e369d-03e2-4097-8a05-b8500e4555bd", "node_type": "1", "metadata": {}, "hash": "2f92361b45d32e7a5d3eb013ae4ad8b3743086288aa5cbb97a7da41940ce232b", "class_name": "RelatedNodeInfo"}}, "text": "2.4 Adding the Second Hidden layer\n\nIn this step, we add another hidden layer\n\n\n2.5 Adding the output layer\n\nIn this step, we add an output layer in our ANN structure output_dim= 1 which means one output node here we use the sigmoid function because our target attribute has a binary class which is one or zero that\u2019s why we use sigmoid activation function.\n\n2.6 Compiling the ANN\n\nIn this step, we compile the ANN to do that we use the compile method and add several parameters the first parameter is optimizer = Adam here use the optimal number of weights. So for choosing the optimal number of weights, there are various algorithms of Stochastic Gradient Descent but very efficient one which is Adam so that\u2019s why we use Adam optimizer here. The second parameter is loss this corresponds to loss function here we use binary_crossentropy because if we see target attribute our dataset which contains the binary value that\u2019s why we choose the binary cross-entropy. The final parameter is metrics basically It\u2019s a list of metrics to be evaluated by the model and here we choose the accuracy metrics.\n\n\n2.7 Fitting the ANN\n\nIn this step we fit the training data our model X_train, y_train is our training data. Here a batch size is basically a number of observations after which you want to update the weights here we take batch size 10. And the final parameter is epoch is basically when whole the training set passed through the ANN here we choose the 100 number of the epoch.\n\n\n\n\n\n\n\n\nPart 3: Making the Prediction and Accuracy Result.\n\n3.1 Predict the test set Result\n\nIn this step, we predict our test set result here our prediction results in probability so we choose 1(customer leave the bank) if the probability is greater than one 0.5 otherwise 0(customer don\u2019t leave the bank).\n\n\n3.2 Confusion metrics\n\nIn this step we make a confusion metric of our test set result to do that we import confusion matrix from sklearn.metrics then in confusion matrix, we pass two parameters first is y_test which is the actual test set result and second is y_pred which predicted the result.\n\n\n3.3 Accuracy Score\n\nIn this step, we calculate the accuracy score based on the actual test result and predict test results.\n\n\nSo here we go we get 84.05% of our ANN model.\n\nIf you want dataset and code you also check my Github Profile.\n\nEnd Notes\nIf you liked this article, be sure to click \u2764 below to recommend it and if you have any questions, leave a comment and I will do my best to answer.\n\nFor being more aware of the world of machine learning, follow me. It\u2019s the best way to find out when I write more articles like this.\n\nYou can also follow me on Github for code & dataset follow on Aacademia.edu for this article, Twitter and Email me directly or find me on LinkedIn. I\u2019d love to hear from you.\n\nThat\u2019s all folks, Have a nice day :)\n\nNeural Networks\nArtificial Neural Network\nMultilayer Perceptron\nBackpropagation\nDeep Learning\n167\n\n\n1\n\n\n\n\nAmir Ali\nThe Art of Data Scicne\nWritten by Amir Ali\n442 Followers\n\u00b7\nEditor for \nThe Art of Data Scicne\n\nMaster's Data Science Graduate @WUT\n\nFollow\n\nMore from Amir Ali and The Art of Data Scicne\nSelf Organizing Map(SOM)\nAmir Ali\nAmir Ali\n\nin\n\nThe Art of Data Scicne\n\nSelf Organizing Map(SOM)\nIn this Chapter of Deep Learning, we will discuss Self Organizing Maps (SOM). It is an Unsupervised Deep Learning technique and we will\u2026\n19 min read\n\u00b7\nMay 26, 2019\n404\n\n11", "start_char_idx": 13934, "end_char_idx": 17333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "447e369d-03e2-4097-8a05-b8500e4555bd": {"__data__": {"id_": "447e369d-03e2-4097-8a05-b8500e4555bd", "embedding": null, "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6442ccb7-7433-4b9f-a3f5-bb92498d8bc2", "node_type": "4", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "9aeb652289c665c3495db0642172227e7fdfc9ba28cc18dc4a861897eba1a716", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cad8024-1554-4ed0-b629-7bf7b1c9e15b", "node_type": "1", "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}, "hash": "2f0ef3bc338372ae00c243380d0517df761c66ad4bb0f469443bd49f500d2368", "class_name": "RelatedNodeInfo"}}, "text": "Boltzmann Machine\nAmir Ali\nAmir Ali\n\nin\n\nThe Art of Data Scicne\n\nBoltzmann Machine\nIn this Chapter of Deep Learning book we will discuss about Boltzmann Machine. It is an Unsupervised Deep Learning technique and we will\u2026\n20 min read\n\u00b7\nMay 26, 2019\n361\n\n12\n\n\n\nDimensionality Reduction(PCA and LDA)\nAmir Ali\nAmir Ali\n\nin\n\nThe Art of Data Scicne\n\nDimensionality Reduction(PCA and LDA)\nIn this Chapter we will discuss about Dimensionality Reduction Algorithms (Principle Component Analysis (PCA) and Linear Discriminant\u2026\n16 min read\n\u00b7\nMar 10, 2019\n331\n\n6\n\n\n\nConvlutional Neural Network(CNN)\nAmir Ali\nAmir Ali\n\nin\n\nThe Art of Data Scicne\n\nConvlutional Neural Network(CNN)\nIn this Chapter of Deep Learning, we will discuss the Convolutional Neural Network. It is a Supervised Deep Learning technique and we will\u2026\n24 min read\n\u00b7\nMay 22, 2019\n183\n\n6\n\n\n\nSee all from Amir Ali\nSee all from The Art of Data Scicne\nRecommended from Medium\nCoding Your First Neural Network FROM SCRATCH\nMansi\nMansi\n\nin\n\nCode Like A Girl\n\nCoding Your First Neural Network FROM SCRATCH\nA step by step guide to building your own Neural Network using NumPy.\n9 min read\n\u00b7\nOct 26, 2023\n118\n\n\n\nUnderstanding Convolutional Neural Networks (CNNs) in Depth\nKoushik\nKoushik\n\nUnderstanding Convolutional Neural Networks (CNNs) in Depth\nConvolutional Neural Networks skillfully capturing and extracting patterns from data, revealing the hidden artistry within pixels.\n12 min read\n\u00b7\nNov 28, 2023\n581\n\n5\n\n\n\nLists\n\n\n\nNatural Language Processing\n1200 stories\n\u00b7\n671 saves\nPrincipal Component Analysis for ML\nTime Series Analysis\ndeep learning cheatsheet for beginner\nPractical Guides to Machine Learning\n10 stories\n\u00b7\n1059 saves\n\n\n\ndata science and AI\n40 stories\n\u00b7\n73 saves\nA row of people in karate uniforms sitting in meditation position\n\n\nStaff Picks\n584 stories\n\u00b7\n755 saves\nMachine Learning 101: Perceptron\nYinghui Liang\nYinghui Liang\n\nMachine Learning 101: Perceptron\nA step-by-step dissection of perceptron algorithm that aims for completely machine learning beginners.\n11 min read\n\u00b7\nOct 21, 2023\n24\n\n\n\nA Brief Introduction to Recurrent Neural Networks\nJonte Dancker\nJonte Dancker\n\nin\n\nTowards Data Science\n\nA Brief Introduction to Recurrent Neural Networks\nAn introduction to RNN, LSTM, and GRU and their implementation\n12 min read\n\u00b7\nDec 26, 2022\n565\n\n7\n\n\n\nConvolutional Neural Network From Scratch\nLu\u00eds Fernando Torres\nLu\u00eds Fernando Torres\n\nin\n\nLatinXinAI\n\nConvolutional Neural Network From Scratch\nThe most effective way of working with image data\n21 min read\n\u00b7\nOct 16, 2023\n732\n\n6\n\n\n\nBuilding a Neural Network from Scratch (with Backpropagation)\nLong Nguyen\nLong Nguyen\n\nBuilding a Neural Network from Scratch (with Backpropagation)\nUnveiling the magic of neural networks: from bare Python to TensorFlow. A hands-on journey to understand and build from scratch\n11 min read\n\u00b7\nNov 18, 2023\n411\n\n6\n\n\n\nSee more recommendations\nHelp\n\nStatus\n\nAbout\n\nCareers\n\nBlog\n\nPrivacy\n\nTerms\n\nText to speech\n\nTeams", "start_char_idx": 17337, "end_char_idx": 20293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"6442ccb7-7433-4b9f-a3f5-bb92498d8bc2": {"node_ids": ["e3565792-87e8-46fd-93a5-a728c849291f", "cb49b3ed-4801-4e0b-812c-8f1548517709", "c231e247-5deb-4758-9e4f-0a16d517e979", "0aa4c12d-2b6b-43f2-90c1-73ca56d44052", "2cad8024-1554-4ed0-b629-7bf7b1c9e15b", "447e369d-03e2-4097-8a05-b8500e4555bd"], "metadata": {"file_path": "data\\doc", "file_name": "doc", "file_type": null, "file_size": 20984, "creation_date": "2024-02-17", "last_modified_date": "2024-02-17", "last_accessed_date": "2024-02-17"}}}}